{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import data_prep\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Temperature lapse rate is:  0.002 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.003 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.004 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.005 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.001 \n",
      "\n",
      ".....................................................................................................................................................................................\n",
      " Elapsed time to run simulation models: 22.044 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_total = time.time()\n",
    "start_time = time.time()\n",
    "# Run the model\n",
    "import Snowmeltmodel_variable\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Elapsed time to run simulation models: {elapsed_time:.3f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002, 0.003, 0.004, 0.005, 0.001]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Assign static features ---- array has length of 1x( all pixels ) x (all simulated versions)\n",
    "dem = Snowmeltmodel_variable.dem_array\n",
    "\n",
    "# Assign dynamic features ---- array has length of (timesteps) x ( all pixels ) x (all simulated versions)\n",
    "snow = Snowmeltmodel_variable.snow_array\n",
    "precipitation = Snowmeltmodel_variable.precipitation_array\n",
    "temp = Snowmeltmodel_variable.temp_array\n",
    "\n",
    "#Retrieve size of frame and timesteps\n",
    "horizontal_pixels  = Snowmeltmodel_variable.horizontal_pixels\n",
    "vertical_pixels = Snowmeltmodel_variable.vertical_pixels\n",
    "timesteps = Snowmeltmodel_variable.timesteps\n",
    "\n",
    "# Retrieve the list and the length of the list of variables used to run the different simulations\n",
    "from Snowmeltmodel_variable import variable_list\n",
    "list_of_variables_for_simulation = variable_list\n",
    "print(list_of_variables_for_simulation)\n",
    "number_of_training_simulations = len(list_of_variables_for_simulation)\n",
    "print(number_of_training_simulations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Assign feature of interest to 'data'\n",
    "data = snow\n",
    "# Repeat Static drivers for each timestep\n",
    "dem = np.repeat(dem, timesteps)\n",
    "\n",
    "list_data = np.array_split(data, number_of_training_simulations)\n",
    "list_precipitation = np.array_split(precipitation, number_of_training_simulations)\n",
    "list_temp = np.array_split(temp, number_of_training_simulations)\n",
    "list_dem = np.array_split(dem, number_of_training_simulations)\n",
    "# create list of drivers and names\n",
    "list_driver_names = ['precipitation','temp', 'dem']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "1\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "2\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "3\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "4\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n"
     ]
    }
   ],
   "source": [
    "dfs = [None] * number_of_training_simulations\n",
    "length_one_simulation = timesteps*vertical_pixels*horizontal_pixels\n",
    "# Add drivers as features\n",
    "for iiii in range(number_of_training_simulations):\n",
    "    print(iiii)\n",
    "\n",
    "    dfs[iiii] = data_prep.only_y_label(data=list_data[iiii],\n",
    "                                               horizontal_pixels=horizontal_pixels,\n",
    "                                               vertical_pixels=vertical_pixels,\n",
    "                                               multiplesteps =False)\n",
    "\n",
    "    list_drivers = [list_precipitation[iiii],list_temp[iiii], list_dem[iiii]]\n",
    "\n",
    "     # Add drivers at each timestep and each pixel to the dataframe\n",
    "    for _ in range(len(list_drivers)):\n",
    "        driver = list_drivers[_]\n",
    "        name = list_driver_names[_]\n",
    "\n",
    "        # Add driver at each timestep and each pixel to the dataframe\n",
    "        dfs[iiii] = data_prep.driver_as_feature(df=dfs[iiii],\n",
    "                                         driver=driver, driver_name=name,\n",
    "                                         horizontal_pixels=horizontal_pixels,\n",
    "                                         vertical_pixels=vertical_pixels,\n",
    "                                         multiplesteps=False)\n",
    "\n",
    "    # Add variable rate as feature\n",
    "    dfs[iiii] = data_prep.VARIABLE_rate_as_feature(df= dfs[iiii],\n",
    "                                                   variable_rate=list_of_variables_for_simulation[iiii],\n",
    "                                                   variable_rate_name='TEMP_rate',\n",
    "                                                   horizontal_pixels=horizontal_pixels,\n",
    "                                                   vertical_pixels=vertical_pixels,\n",
    "                                                   multiplesteps=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_input', 'precipitation', 'temp', 'dem', 'TEMP_rate']\n",
      "(1074000, 5)\n",
      "y_label\n",
      "(1074000,)\n",
      "1074000\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate all dfs\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Name features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Extract feature names for later processing\n",
    "colnames = list(df.columns.values.tolist())\n",
    "feature_names = colnames[:-1]\n",
    "label_name = colnames[-1]\n",
    "\n",
    "# Check data prep\n",
    "print(feature_names)\n",
    "print(features.shape)\n",
    "print(label_name)\n",
    "print(labels.shape)\n",
    "print(number_of_training_simulations*horizontal_pixels*vertical_pixels*timesteps-(number_of_training_simulations*horizontal_pixels*vertical_pixels*2))\n",
    "\n",
    "if labels.shape[0] == number_of_training_simulations*horizontal_pixels*vertical_pixels*timesteps-(number_of_training_simulations*horizontal_pixels*vertical_pixels*2):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('CHECK DATA PREP')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   x_input  precipitation      temp    dem  TEMP_rate  y_label\n0      0.0       0.000873  4.932301  878.0      0.002      0.0\n1      0.0       0.000873  4.320300  878.0      0.002      0.0\n2      0.0       0.000873  4.320300  878.0      0.002      0.0\n3      0.0       0.000873  3.798300  878.0      0.002      0.0\n4      0.0       0.000873  4.794300  878.0      0.002      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_input</th>\n      <th>precipitation</th>\n      <th>temp</th>\n      <th>dem</th>\n      <th>TEMP_rate</th>\n      <th>y_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000873</td>\n      <td>4.932301</td>\n      <td>878.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.000873</td>\n      <td>4.320300</td>\n      <td>878.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.000873</td>\n      <td>4.320300</td>\n      <td>878.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.000873</td>\n      <td>3.798300</td>\n      <td>878.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.000873</td>\n      <td>4.794300</td>\n      <td>878.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Split train/test\n",
    "\n",
    "length_one_simulation = timesteps*vertical_pixels*horizontal_pixels\n",
    "one_time_step = horizontal_pixels*vertical_pixels\n",
    "\n",
    "# Split at last timestep to predict last timestep\n",
    "X_train = features.iloc[:-(length_one_simulation), :]\n",
    "y_train = labels.iloc[:-(length_one_simulation)]\n",
    "\n",
    "# The test set only contains the first initialised values for the simulation. The rest is predicted\n",
    "X_test = features.iloc[-(length_one_simulation):(-(length_one_simulation)+one_time_step), :]\n",
    "\n",
    "# Since we want to predict the entire simulation we do not have a set ylabel. Perhaps later construct y_label from all following simulated steps. But time series of important points generates a better view of the performance\n",
    "y_test =  labels.iloc[-(length_one_simulation)+one_time_step:]\n",
    "\n",
    "# Check train/test\n",
    "print('X_train ', X_train.shape) #training features\n",
    "print('Y_train ', y_train.shape) #training labels\n",
    "\n",
    "print('X_test (only 1st step of last simulation)', X_test.shape) #testing features\n",
    "print('Y_test (multiplestep Y>>X, complete simulation)', y_test.shape) #testing labels\n",
    "\n",
    "print('Features ', features.shape) #total just to check if testing is really not inside training. shapes add up\n",
    "print('Labels ', labels.shape)\n",
    "\n",
    "optimise_model_true = False\n",
    "\n",
    "if optimise_model_true:\n",
    "    # Altogether, there are many possible settings.\n",
    "    # However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n",
    "\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start=10, stop=500, num=8)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = [1.0, 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(1, 200, num=10)]\n",
    "\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 4, 6]\n",
    "\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [4, 8, 16]\n",
    "\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Random search of parameters, using 3 fold cross validation,\n",
    "    # search across 400 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator=rf,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=100,\n",
    "                                   cv=3,\n",
    "                                   verbose=4,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    print(rf_random.best_params_)\n",
    "\n",
    "else:\n",
    "    # fit the model\n",
    "    rf = RandomForestRegressor(n_estimators=290,\n",
    "                               min_samples_split=2,\n",
    "                               min_samples_leaf=4,\n",
    "                               max_features=1.0,\n",
    "                               max_depth=67,\n",
    "                               bootstrap=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict one complete simulation on unseen variable\n",
    "list_drivers_once = [list_precipitation[0],list_temp[0], list_dem[0]]\n",
    "\n",
    "# -1 timestep due to removed last timesteps to avoid NaN's in Labels\n",
    "steps = timesteps-1\n",
    "\n",
    "# Initialise for saving predictions for animation and performance measures, respectively.\n",
    "framed_predictions = pd.DataFrame()\n",
    "array_predictions = np.empty(0)\n",
    "list_each_prediction = [None] *steps\n",
    "\n",
    "#Choose the initial starting state of the model as the Test set\n",
    "X_test_multiple = X_train.iloc[:(horizontal_pixels*vertical_pixels),:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# - Make predictions\n",
    "# - Save prediction\n",
    "# - Set prediction as new input\n",
    "#\n",
    "# - Add static and dynamic drivers to new input\n",
    "last_pixel_driver = 0\n",
    "for _ in range(steps):\n",
    "    print(_)\n",
    "    # Prediction step\n",
    "    y_pred_rf = rf.predict(X_test_multiple)\n",
    "\n",
    "    # Save the solution as a dataframe of pixels\n",
    "    # For animation\n",
    "    projected_prediction = pd.DataFrame(np.reshape(y_pred_rf,(int(vertical_pixels),int(horizontal_pixels))))\n",
    "    framed_predictions = pd.concat([framed_predictions, projected_prediction], axis=0)\n",
    "\n",
    "    # For MSE map\n",
    "    list_each_prediction[_] = projected_prediction\n",
    "\n",
    "    # For performance measures\n",
    "    array_predictions = np.append(array_predictions, y_pred_rf)\n",
    "\n",
    "    # Create the new test set for next prediction\n",
    "    new_state = data_prep.only_y_label(y_pred_rf.reshape(-1,1),\n",
    "                                               horizontal_pixels,\n",
    "                                               vertical_pixels,\n",
    "                                               multiplesteps=True,\n",
    "                                               print_true= False)\n",
    "    # Add the drivers at that timestep\n",
    "    first_pixel_driver = last_pixel_driver\n",
    "    last_pixel_driver = first_pixel_driver + (vertical_pixels*horizontal_pixels)\n",
    "    for __ in range(len(list_drivers_once)):\n",
    "        driver = list_drivers_once[__]\n",
    "        name = list_driver_names[__]\n",
    "        new_state = data_prep.driver_as_feature(df = new_state,\n",
    "                                                driver=driver[first_pixel_driver:(last_pixel_driver)],\n",
    "                                                driver_name= name,\n",
    "                                                horizontal_pixels=horizontal_pixels,\n",
    "                                                vertical_pixels=vertical_pixels,\n",
    "                                                multiplesteps=True)\n",
    "\n",
    "    # Add variable rate as feature\n",
    "    df = data_prep.VARIABLE_rate_as_feature(df = new_state,\n",
    "                                            variable_rate= list_of_variables_for_simulation[-1],\n",
    "                                            variable_rate_name = 'TEMP_rate',\n",
    "                                            horizontal_pixels=horizontal_pixels,\n",
    "                                            vertical_pixels=vertical_pixels,\n",
    "                                            multiplesteps=True)\n",
    "\n",
    "\n",
    "    # remove the Y_label-- y_label only used for training\n",
    "    X_test_multiple = new_state.iloc[:, :-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "# Generate results:\n",
    "# MSE_map: Mean of each pixel averaged over all timesteps\n",
    "\n",
    "# Reshape y_test for MSE. Same shape as framed predictions. vertical x horziontal with next timestep below.\n",
    "framed_y_test = pd.DataFrame()\n",
    "first_pixel_y_test = 0\n",
    "y_test = pd.Series(y_test)\n",
    "list_y_test_ = [None] * steps\n",
    "\n",
    "for _ in range(steps):\n",
    "\n",
    "    last_pixel_y_test = one_time_step*(_+1)\n",
    "    # print(\"last pixel = \",last_pixel_y_test)\n",
    "    # print(\"first pixel = \", first_pixel_y_test)\n",
    "    # print('length y_test = ', len(y_test))\n",
    "    # print('length y_test[i:j] = ', len(y_test[first_pixel_y_test:last_pixel_y_test]))\n",
    "    projected_y_test = pd.DataFrame(y_test[first_pixel_y_test:last_pixel_y_test].values.reshape((int(vertical_pixels),int(horizontal_pixels))))\n",
    "    list_y_test_[_] = projected_y_test\n",
    "    framed_y_test = pd.concat([framed_y_test, projected_y_test], axis=0)\n",
    "\n",
    "    first_pixel_y_test = last_pixel_y_test\n",
    "\n",
    "squared_difference = [None] * steps\n",
    "test_concat = pd.DataFrame()\n",
    "for __ in range(steps):\n",
    "    # print(__)\n",
    "    squared_difference[__] = (list_each_prediction[__] - list_y_test_[__])**2\n",
    "\n",
    "dfs = squared_difference\n",
    "MSE_map = pd.concat([each.stack() for each in dfs],axis=1)\\\n",
    "             .apply(lambda x:x.mean(),axis=1)\\\n",
    "             .unstack()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "cmap = cm.coolwarm\n",
    "im = ax.imshow(MSE_map, interpolation='nearest', cmap=cmap, vmin=MSE_map.to_numpy().max(), vmax=MSE_map.to_numpy().min())\n",
    "ax.set_title('MSE distribution \\n  temprate = 0.004')\n",
    "fig.colorbar(im,cax=cax, orientation='vertical', extend = 'both')#, ticks= [0.05,0.1,0.15,0.2,0.25])\n",
    "\n",
    "plt.plot()\n",
    "plt.savefig('Results/_rate004_MSE_map.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# overall MSE/MAPE/MAE/Explained variance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, explained_variance_score,max_error\n",
    "\n",
    "# Mean absolute percentage error (MAPE) regression loss.\n",
    "#\n",
    "# Note here that the output is not a percentage in the range [0, 100] and a value of 100 does not mean 100% but 1e2. Furthermore, the output can be arbitrarily high when y_true is small (which is specific to the metric) or when abs(y_true - y_pred) is large (which is common for most regression metrics).\n",
    "\n",
    "\n",
    "MAPE = mean_absolute_percentage_error(y_true=y_test, y_pred = array_predictions)\n",
    "MAE = mean_absolute_error(y_true=y_test, y_pred = array_predictions)\n",
    "MSE = mean_squared_error(y_true=y_test, y_pred = array_predictions)\n",
    "MAX = max_error(y_true=y_test, y_pred = array_predictions)\n",
    "EXPL_VAR = explained_variance_score(y_true=y_test, y_pred = array_predictions)\n",
    "\n",
    "# print('mean absolute percentage: ', MAPE) Too small y_true values for this measure??\n",
    "print('mean absolute error: ',MAE)\n",
    "print('mean squared error: ',MSE)\n",
    "print('MAX ERROR: ',MAX)\n",
    "print('Explained variance score: ',EXPL_VAR)\n",
    "\n",
    "# Visualise predictions\n",
    "\n",
    "# Initialise the points\n",
    "point_1 = []\n",
    "point_2 = []\n",
    "point_3 = []\n",
    "point_4 = []\n",
    "point_1_pred = []\n",
    "point_2_pred = []\n",
    "point_3_pred = []\n",
    "point_4_pred = []\n",
    "\n",
    "# Choose the points of interest\n",
    "point_1_in_array, point_2_in_array, point_3_in_array, point_4_in_array = 139, 261, 755, 863\n",
    "\n",
    "# Extract the timeseries data of these points from the simulated and emulated data\n",
    "plot_xvalues = timesteps-2\n",
    "for x in range(plot_xvalues):\n",
    "    point_1_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_2_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_3_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_4_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_1 = np.append(point_1, snow[point_1_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_2 = np.append(point_2, snow[point_2_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_3 = np.append(point_3, snow[point_3_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_4 = np.append(point_4, snow[point_4_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_1_pred = np.append(point_1_pred, array_predictions[point_1_in_array])\n",
    "    point_2_pred = np.append(point_2_pred, array_predictions[point_2_in_array])\n",
    "    point_3_pred = np.append(point_3_pred, array_predictions[point_3_in_array])\n",
    "    point_4_pred = np.append(point_4_pred, array_predictions[point_4_in_array])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4)= plt.subplots(4, 1, figsize=(15,15))\n",
    "fig.suptitle('Emulation performance on several points of the simulation', fontsize=18)\n",
    "\n",
    "#---- point 1\n",
    "ax1.plot(range(plot_xvalues), point_1, '.-', color = 'green', linewidth= 2)\n",
    "ax1.plot(range(plot_xvalues), point_1_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 2\n",
    "ax2.plot(range(plot_xvalues), point_2, '.-', color = 'green', linewidth= 2)\n",
    "ax2.plot(range(plot_xvalues), point_2_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 3\n",
    "ax3.plot(range(plot_xvalues), point_3, '.-', color = 'green', linewidth= 2)\n",
    "ax3.plot(range(plot_xvalues), point_3_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 4\n",
    "ax4.plot(range(plot_xvalues), point_4, '.-', color = 'green', linewidth= 2)\n",
    "ax4.plot(range(plot_xvalues), point_4_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "ax1.set_title('First Point', fontsize = 14)\n",
    "ax1.set_ylabel('Snowfall [m]')\n",
    "ax1.set_xlabel('Timestep [day]')\n",
    "ax2.set_title('Second Point', fontsize = 14)\n",
    "ax2.set_ylabel('Snowfall [m]')\n",
    "ax2.set_xlabel('Timestep [day]')\n",
    "ax3.set_title('Third Point', fontsize = 14)\n",
    "ax3.set_ylabel('Snowfall [m]')\n",
    "ax3.set_xlabel('Timestep [day]')\n",
    "ax4.set_title('Fourth Point', fontsize = 14)\n",
    "ax4.set_ylabel('Snowfall [m]')\n",
    "ax4.set_xlabel('Timestep [day]')\n",
    "\n",
    "ax1.legend(['target', 'predicted'])\n",
    "ax2.legend(['target', 'predicted'])\n",
    "ax3.legend(['target', 'predicted'])\n",
    "ax4.legend(['target', 'predicted'])\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()\n",
    "ax4.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.subplot_tool()\n",
    "plt.plot()\n",
    "plt.savefig('Results/_rate004_timeseries.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Autocorrelation plot for pattern evaluation\n",
    "fig, (ax1, ax2, ax3, ax4)= plt.subplots(4, 1, figsize=(15,15))\n",
    "fig.suptitle('Autocorrelation of simulation and emulation on several points', fontsize=18)\n",
    "\n",
    "#---- point 1\n",
    "ax1.acorr(point_1, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax1.acorr(point_1_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 2\n",
    "ax2.acorr(point_2, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax2.acorr(point_2_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 3\n",
    "ax3.acorr(point_3, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax3.acorr(point_3_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 4\n",
    "ax4.acorr(point_4, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax4.acorr(point_4_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "ax1.set_title('First Point', fontsize = 14)\n",
    "ax2.set_title('Second Point', fontsize = 14)\n",
    "ax3.set_title('Third Point', fontsize = 14)\n",
    "ax4.set_title('Fourth Point', fontsize = 14)\n",
    "\n",
    "\n",
    "ax1.legend(['target', 'predicted'] )\n",
    "ax2.legend(['target', 'predicted'])\n",
    "ax3.legend(['target', 'predicted'])\n",
    "ax4.legend(['target', 'predicted'])\n",
    "\n",
    "\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.subplot_tool()\n",
    "plt.plot()\n",
    "plt.savefig('Results/_rate004_autocorrelation.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "elapsed_time = time.time() - start_time_total\n",
    "\n",
    "print(f\"Elapsed time to run whole script: {elapsed_time:.3f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
