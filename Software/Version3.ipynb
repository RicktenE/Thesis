{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Prediction visualisation\n",
    "import matplotlib.cm as cm # Used later on, has to be included in codeblock where it is used due to same name of functions in other packages\n",
    "\n",
    "\n",
    "# Tree Visualisation\n",
    "import os\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# !!! make sure that the files are in the same folder\n",
    "import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run and import the simulation model with a predefined range of parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters are defined in the Snowmeltmodel_variable.py document.\n",
    "A possible next extention could be to make this more user friendly by asking for inputs such as with the Game_of_Life.py simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Temperature lapse rate is:  0.002 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.003 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.004 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.005 \n",
      "\n",
      "..................................................................................................................................................................................... \n",
      " Temperature lapse rate is:  0.001 \n",
      "\n",
      "....................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "import Snowmeltmodel_variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Assign static features ---- array has length of 1x( all pixels ) x (all simulated versions)\n",
    "dem = Snowmeltmodel_variable.dem_array\n",
    "\n",
    "# Assign dynamic features ---- array has length of (timesteps) x ( all pixels ) x (all simulated versions)\n",
    "snow = Snowmeltmodel_variable.snow_array\n",
    "precipitation = Snowmeltmodel_variable.precipitation_array\n",
    "temp = Snowmeltmodel_variable.temp_array\n",
    "snowfall = Snowmeltmodel_variable.snowfall_array\n",
    "rainfall = Snowmeltmodel_variable.rainfall_array\n",
    "actualmelt = Snowmeltmodel_variable.actualmelt_array\n",
    "runoff = Snowmeltmodel_variable.runoff_array\n",
    "\n",
    "#Retrieve size of frame and timesteps\n",
    "horizontal_pixels  = Snowmeltmodel_variable.horizontal_pixels\n",
    "vertical_pixels = Snowmeltmodel_variable.vertical_pixels\n",
    "timesteps = Snowmeltmodel_variable.timesteps\n",
    "\n",
    "# Retrieve the list and the length of the list of variables used to run the different simulations\n",
    "list_of_variables_for_simulation = Snowmeltmodel_variable.variable_list\n",
    "number_of_training_simulations = len(list_of_variables_for_simulation)\n",
    "print(number_of_training_simulations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Assign feature of interest to 'data'\n",
    "data = snow\n",
    "# Repeat Static drivers for each timestep\n",
    "dem = np.repeat(dem, timesteps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Due to variable rates, multiple simualated trainings.\n",
    "Cut up arrays for each simulation. Add the new columns, such as neighbours and drivers.\n",
    "Append all complete simulations to one dataframe for later processing\n",
    "\n",
    "\n",
    "!! Also for the drivers since we take away the last simulation step because there are no labels to train on this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "list_data = np.array_split(data, number_of_training_simulations)\n",
    "list_precipitation = np.array_split(precipitation, number_of_training_simulations)\n",
    "list_temp = np.array_split(temp, number_of_training_simulations)\n",
    "list_snowfall = np.array_split(snowfall, number_of_training_simulations)\n",
    "list_rainfall = np.array_split(rainfall, number_of_training_simulations)\n",
    "list_actualmelt = np.array_split(actualmelt, number_of_training_simulations)\n",
    "list_runoff = np.array_split(runoff, number_of_training_simulations)\n",
    "list_dem = np.array_split(dem, number_of_training_simulations)\n",
    "# create list of drivers and names\n",
    "list_driver_names = ['precipitation','temp', 'dem']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loop over number of training simulations to add:\n",
    "Neighbours as features\n",
    "Drivers as features\n",
    "\n",
    "Results in X number of dataframes. Later append dataframes for training data.\n",
    "where X = number of training simulations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "1\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "2\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "3\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n",
      "4\n",
      "Total number of data points :  216000\n",
      "Length of one row of pixels, horizontal side of the grid:  30\n"
     ]
    }
   ],
   "source": [
    "dfs = [None] * number_of_training_simulations\n",
    "\n",
    "for iiii in range(number_of_training_simulations):\n",
    "    print(iiii)\n",
    "     # Extract the neighbours as features\n",
    "    # Set the results of the next timestep as labels for each timestep.\n",
    "    # Remove the last ran timestep of each simulation to avoid NaN's\n",
    "\n",
    "    dfs[iiii] = data_prep.neighbour_as_feature(data=list_data[iiii],\n",
    "                                               horizontal_pixels=horizontal_pixels,\n",
    "                                               vertical_pixels=vertical_pixels,\n",
    "                                               multiplesteps =False)\n",
    "\n",
    "    list_drivers = [list_precipitation[iiii],list_temp[iiii], list_dem[iiii]]\n",
    "\n",
    "\n",
    "\n",
    "     # Add drivers at each timestep and each pixel to the dataframe\n",
    "    for _ in range(len(list_drivers)):\n",
    "        driver = list_drivers[_]\n",
    "        name = list_driver_names[_]\n",
    "\n",
    "        # Add driver at each timestep and each pixel to the dataframe\n",
    "        dfs[iiii] = data_prep.driver_as_feature(df=dfs[iiii],\n",
    "                                         driver=driver, driver_name=name,\n",
    "                                         horizontal_pixels=horizontal_pixels,\n",
    "                                         vertical_pixels=vertical_pixels,\n",
    "                                         multiplesteps=False)\n",
    "\n",
    "    # Add variable rate as feature\n",
    "    dfs[iiii] = data_prep.VARIABLE_rate_as_feature(df= dfs[iiii],\n",
    "                                                   variable_rate=list_of_variables_for_simulation[iiii],\n",
    "                                                   variable_rate_name='TEMP_rate',\n",
    "                                                   horizontal_pixels=horizontal_pixels,\n",
    "                                                   vertical_pixels=vertical_pixels,\n",
    "                                                   multiplesteps=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concatenate all dfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "\n",
    "# Name features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Extract feature names for later processing\n",
    "colnames = list(df.columns.values.tolist())\n",
    "feature_names = colnames[:-1]\n",
    "label_name = colnames[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_input', 'left', 'top_left', 'top', 'top_right', 'right', 'bottom_right', 'bottom', 'bottom_left', 'precipitation', 'temp', 'dem', 'TEMP_rate']\n",
      "(1074000, 13)\n",
      "y_label\n",
      "(1074000,)\n",
      "1074000\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)\n",
    "print(features.shape)\n",
    "print(label_name)\n",
    "print(labels.shape)\n",
    "print(number_of_training_simulations*horizontal_pixels*vertical_pixels*timesteps-(number_of_training_simulations*horizontal_pixels*vertical_pixels*2))\n",
    "\n",
    "if labels.shape[0] == number_of_training_simulations*horizontal_pixels*vertical_pixels*timesteps-(number_of_training_simulations*horizontal_pixels*vertical_pixels*2):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('CHECK DATA PREP')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train on simulations except the last one (made in such a way that the last is the desired one)\n",
    "#### Test on simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "length_one_simulation = timesteps*vertical_pixels*horizontal_pixels\n",
    "one_time_step = horizontal_pixels*vertical_pixels\n",
    "\n",
    "# Split at last timestep to predict last timestep\n",
    "X_train = features.iloc[:-(length_one_simulation), :]\n",
    "y_train = labels.iloc[:-(length_one_simulation)]\n",
    "\n",
    "# The test set only contains the first initialised values for the simulation. The rest is predicted\n",
    "X_test = features.iloc[-(length_one_simulation):(-(length_one_simulation)+one_time_step), :]\n",
    "\n",
    "# Since we want to predict the entire simulation we do not have a set ylabel. Perhaps later construct y_label from all following simulated steps. But time series of important points generates a better view of the performance\n",
    "y_test =  labels.iloc[-(length_one_simulation)+one_time_step:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check train/test split shapes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  (856800, 13)\n",
      "Y_train  (856800,)\n",
      "X_test (only 1st step of last simulation) (1200, 13)\n",
      "Y_test (multiplestep Y>>X, complete simulation) (216000,)\n",
      "Features  (1074000, 13)\n",
      "Labels  (1074000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train ', X_train.shape) #training features\n",
    "print('Y_train ', y_train.shape) #training labels\n",
    "\n",
    "print('X_test (only 1st step of last simulation)', X_test.shape) #testing features\n",
    "print('Y_test (multiplestep Y>>X, complete simulation)', y_test.shape) #testing labels\n",
    "\n",
    "print('Features ', features.shape) #total just to check if testing is really not inside training. shapes add up\n",
    "print('Labels ', labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model optimisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 80, 150, 220, 290, 360, 430, 500], 'max_features': [1.0, 'sqrt'], 'max_depth': [1, 23, 45, 67, 89, 111, 133, 155, 177, 200], 'min_samples_split': [2, 4, 6], 'min_samples_leaf': [4, 8, 16], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    " # Altogether, there are many possible settings.\n",
    "# However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 8)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [1.0 , 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 200, num = 10)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,4,6]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [4, 8, 16]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### perform hyper parameter optimisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=100,\n                   n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'max_depth': [1, 23, 45, 67, 89, 111,\n                                                      133, 155, 177, 200],\n                                        'max_features': [1.0, 'sqrt'],\n                                        'min_samples_leaf': [4, 8, 16],\n                                        'min_samples_split': [2, 4, 6],\n                                        'n_estimators': [10, 80, 150, 220, 290,\n                                                         360, 430, 500]},\n                   random_state=42, verbose=4)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=100,\n                   n_jobs=-1,\n                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n                                        &#x27;max_depth&#x27;: [1, 23, 45, 67, 89, 111,\n                                                      133, 155, 177, 200],\n                                        &#x27;max_features&#x27;: [1.0, &#x27;sqrt&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: [4, 8, 16],\n                                        &#x27;min_samples_split&#x27;: [2, 4, 6],\n                                        &#x27;n_estimators&#x27;: [10, 80, 150, 220, 290,\n                                                         360, 430, 500]},\n                   random_state=42, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=100,\n                   n_jobs=-1,\n                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n                                        &#x27;max_depth&#x27;: [1, 23, 45, 67, 89, 111,\n                                                      133, 155, 177, 200],\n                                        &#x27;max_features&#x27;: [1.0, &#x27;sqrt&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: [4, 8, 16],\n                                        &#x27;min_samples_split&#x27;: [2, 4, 6],\n                                        &#x27;n_estimators&#x27;: [10, 80, 150, 220, 290,\n                                                         360, 430, 500]},\n                   random_state=42, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 400 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100,\n",
    "                               cv = 3,\n",
    "                               verbose=4,\n",
    "                               random_state=42,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'n_estimators': 290,\n 'min_samples_split': 2,\n 'min_samples_leaf': 4,\n 'max_features': 1.0,\n 'max_depth': 67,\n 'bootstrap': True}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators =  290,\n",
    "                           min_samples_split = 2,\n",
    "                           min_samples_leaf = 4,\n",
    "                           max_features = 1.0,\n",
    "                           max_depth =  67,\n",
    "                           bootstrap = True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict one complete simulation -- add correct driver at each timestep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remake list_drivers_once to include all drivers just once. Drivers did not change over different simulations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list_drivers_once = [list_precipitation[0],list_temp[0],list_snowfall[0],list_rainfall[0],list_actualmelt[0],list_runoff[0], list_dem[0]]\n",
    "list_drivers_once = [list_precipitation[0],list_temp[0], list_dem[0]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -1 timestep due to removed last timesteps to avoid NaN's in Labels\n",
    "steps = timesteps-1\n",
    "\n",
    "# Initialise for saving predictions for animation and performance measures, respectively.\n",
    "framed_predictions = pd.DataFrame()\n",
    "array_predictions = np.empty(0)\n",
    "\n",
    "#Choose the initial starting state of the model as the Test set\n",
    "X_test_multiple = X_train.iloc[:(horizontal_pixels*vertical_pixels),:]\n",
    "\n",
    "# - Make predictions\n",
    "# - Save prediction\n",
    "# - Set prediction as new input\n",
    "# - Set neighbours for new input\n",
    "# - Add static and dynamic drivers to new input\n",
    "last_pixel_driver = 0\n",
    "for _ in range(steps):\n",
    "    print(_)\n",
    "    # Prediction step\n",
    "    y_pred_rf = rf.predict(X_test_multiple)\n",
    "\n",
    "    # Save the solution as a dataframe of pixels\n",
    "    # For animation\n",
    "    projected_prediction = pd.DataFrame(np.reshape(y_pred_rf,(int(vertical_pixels),int(horizontal_pixels))))\n",
    "    framed_predictions = pd.concat([framed_predictions, projected_prediction], axis=0)\n",
    "\n",
    "    # For performance measures\n",
    "    array_predictions = np.append(array_predictions, y_pred_rf)\n",
    "\n",
    "\n",
    "    # Create the new test set for next prediction\n",
    "    new_state = data_prep.neighbour_as_feature(y_pred_rf.reshape(-1,1),\n",
    "                                               horizontal_pixels,\n",
    "                                               vertical_pixels,\n",
    "                                               multiplesteps=True,\n",
    "                                               print_true= False)\n",
    "\n",
    "    # Add the drivers at that timestep\n",
    "    first_pixel_driver = last_pixel_driver\n",
    "    last_pixel_driver = first_pixel_driver + (vertical_pixels*horizontal_pixels)\n",
    "    for __ in range(len(list_drivers_once)):\n",
    "        driver = list_drivers_once[__]\n",
    "        name = list_driver_names[__]\n",
    "        new_state = data_prep.driver_as_feature(df = new_state,\n",
    "                                                driver=driver[first_pixel_driver:(last_pixel_driver)],\n",
    "                                                driver_name= name,\n",
    "                                                horizontal_pixels=horizontal_pixels,\n",
    "                                                vertical_pixels=vertical_pixels,\n",
    "                                                multiplesteps=True)\n",
    "\n",
    "    # Add variable rate as feature\n",
    "    df = data_prep.VARIABLE_rate_as_feature(df = new_state,\n",
    "                                            variable_rate= list_of_variables_for_simulation[-1],\n",
    "                                            variable_rate_name = 'TEMP_rate',\n",
    "                                            horizontal_pixels=horizontal_pixels,\n",
    "                                            vertical_pixels=vertical_pixels,\n",
    "                                            multiplesteps=True)\n",
    "\n",
    "    # # Check if the driver is added properly\n",
    "    # colnames = list(df.columns.values.tolist())\n",
    "    # print(colnames)\n",
    "    # if _ == 50:\n",
    "    #     print(new_state)\n",
    "\n",
    "\n",
    "    # remove the Y_label-- y_label only used for training\n",
    "    X_test_multiple = new_state.iloc[:, :-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create an animation of the MSE spread over the map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Average error, accuracy. Not a fitting performance measure for these results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(predictions, test_labels):\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(predictions=array_predictions, test_labels=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MAE, MAPE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, explained_variance_score,max_error\n",
    "\n",
    "# Mean absolute percentage error (MAPE) regression loss.\n",
    "#\n",
    "# Note here that the output is not a percentage in the range [0, 100] and a value of 100 does not mean 100% but 1e2. Furthermore, the output can be arbitrarily high when y_true is small (which is specific to the metric) or when abs(y_true - y_pred) is large (which is common for most regression metrics).\n",
    "\n",
    "\n",
    "MAPE = mean_absolute_percentage_error(y_true=y_test, y_pred = array_predictions)\n",
    "MAE = mean_absolute_error(y_true=y_test, y_pred = array_predictions)\n",
    "MSE = mean_squared_error(y_true=y_test, y_pred = array_predictions)\n",
    "MAX = max_error(y_true=y_test, y_pred = array_predictions)\n",
    "EXPL_VAR = explained_variance_score(y_true=y_test, y_pred = array_predictions)\n",
    "\n",
    "# print('mean absolute percentage: ', MAPE) Too small y_true values for this measure??\n",
    "print('mean absolute error: ',MAE)\n",
    "print('mean squared error: ',MSE)\n",
    "print('MAX ERROR: ',MAX)\n",
    "print('Explained variance score: ',EXPL_VAR)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Timeseries of four points of interest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise the points\n",
    "point_1 = []\n",
    "point_2 = []\n",
    "point_3 = []\n",
    "point_4 = []\n",
    "point_1_pred = []\n",
    "point_2_pred = []\n",
    "point_3_pred = []\n",
    "point_4_pred = []\n",
    "\n",
    "# Choose the points of interest\n",
    "point_1_in_array, point_2_in_array, point_3_in_array, point_4_in_array = 139, 261, 755, 863\n",
    "\n",
    "# Extract the timeseries data of these points from the simulated and emulated data\n",
    "plot_xvalues = timesteps-2\n",
    "for x in range(plot_xvalues):\n",
    "    point_1_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_2_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_3_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_4_in_array += int(vertical_pixels*horizontal_pixels)\n",
    "    point_1 = np.append(point_1, snow[point_1_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_2 = np.append(point_2, snow[point_2_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_3 = np.append(point_3, snow[point_3_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_4 = np.append(point_4, snow[point_4_in_array+((number_of_training_simulations-1)*length_one_simulation)])\n",
    "    point_1_pred = np.append(point_1_pred, array_predictions[point_1_in_array])\n",
    "    point_2_pred = np.append(point_2_pred, array_predictions[point_2_in_array])\n",
    "    point_3_pred = np.append(point_3_pred, array_predictions[point_3_in_array])\n",
    "    point_4_pred = np.append(point_4_pred, array_predictions[point_4_in_array])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the timeseries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2, ax3, ax4)= plt.subplots(4, 1, figsize=(15,15))\n",
    "fig.suptitle('Emulation performance on several points of the simulation', fontsize=18)\n",
    "\n",
    "#---- point 1\n",
    "ax1.plot(range(plot_xvalues), point_1, '.-', color = 'green', linewidth= 2)\n",
    "ax1.plot(range(plot_xvalues), point_1_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 2\n",
    "ax2.plot(range(plot_xvalues), point_2, '.-', color = 'green', linewidth= 2)\n",
    "ax2.plot(range(plot_xvalues), point_2_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 3\n",
    "ax3.plot(range(plot_xvalues), point_3, '.-', color = 'green', linewidth= 2)\n",
    "ax3.plot(range(plot_xvalues), point_3_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "#---- point 4\n",
    "ax4.plot(range(plot_xvalues), point_4, '.-', color = 'green', linewidth= 2)\n",
    "ax4.plot(range(plot_xvalues), point_4_pred, '.-', color = 'blue', linewidth= 1.0)\n",
    "\n",
    "ax1.set_title('First Point', fontsize = 14)\n",
    "ax1.set_ylabel('Snowfall [m]')\n",
    "ax1.set_xlabel('Timestep [day]')\n",
    "ax2.set_title('Second Point', fontsize = 14)\n",
    "ax2.set_ylabel('Snowfall [m]')\n",
    "ax2.set_xlabel('Timestep [day]')\n",
    "ax3.set_title('Third Point', fontsize = 14)\n",
    "ax3.set_ylabel('Snowfall [m]')\n",
    "ax3.set_xlabel('Timestep [day]')\n",
    "ax4.set_title('Fourth Point', fontsize = 14)\n",
    "ax4.set_ylabel('Snowfall [m]')\n",
    "ax4.set_xlabel('Timestep [day]')\n",
    "\n",
    "ax1.legend(['target', 'predicted'])\n",
    "ax2.legend(['target', 'predicted'])\n",
    "ax3.legend(['target', 'predicted'])\n",
    "ax4.legend(['target', 'predicted'])\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()\n",
    "ax4.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.subplot_tool()\n",
    "plt.plot()\n",
    "plt.savefig('unseen_timeseries.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross correlation plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross correlation plot shows the lags of two time series with each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as stattools\n",
    "\n",
    "# Compute Cross Correlations\n",
    "ccs = stattools.ccf(point_4, point_4_pred)[:plot_xvalues]\n",
    "nlags = len(ccs)\n",
    "\n",
    "# Compute the Significance level\n",
    "conf_level = 2 / np.sqrt(nlags)\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.hlines(0, xmin=0, xmax=plot_xvalues, color='gray')  # 0 axis\n",
    "plt.hlines(conf_level, xmin=0, xmax=plot_xvalues, color='gray')\n",
    "plt.hlines(-conf_level, xmin=0, xmax=plot_xvalues, color='gray')\n",
    "\n",
    "plt.bar(x=np.arange(len(ccs)), height=ccs, width=.3)\n",
    "\n",
    "# Decoration\n",
    "plt.title('$Cross\\; Correlation\\; Plot:\\;$ \\n $ simulated \\; vs\\; emulated$', fontsize=22)\n",
    "plt.xlim(0,len(ccs))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4)= plt.subplots(4, 1, figsize=(15,15))\n",
    "fig.suptitle('Autocorrelation of simulation and emulation on several points', fontsize=18)\n",
    "\n",
    "#---- point 1\n",
    "ax1.acorr(point_1, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax1.acorr(point_1_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 2\n",
    "ax2.acorr(point_2, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax2.acorr(point_2_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 3\n",
    "ax3.acorr(point_3, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax3.acorr(point_3_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "#---- point 4\n",
    "ax4.acorr(point_4, maxlags = plot_xvalues-1, color = 'blue', alpha = 1.0)\n",
    "ax4.acorr(point_4_pred, maxlags = plot_xvalues-1, color = 'red', alpha=0.4, linestyle = '-')\n",
    "\n",
    "ax1.set_title('First Point', fontsize = 14)\n",
    "ax2.set_title('Second Point', fontsize = 14)\n",
    "ax3.set_title('Third Point', fontsize = 14)\n",
    "ax4.set_title('Fourth Point', fontsize = 14)\n",
    "\n",
    "\n",
    "ax1.legend(['target', 'predicted'] )\n",
    "ax2.legend(['target', 'predicted'])\n",
    "ax3.legend(['target', 'predicted'])\n",
    "ax4.legend(['target', 'predicted'])\n",
    "\n",
    "# ax1.grid()\n",
    "# ax2.grid()\n",
    "# ax3.grid()\n",
    "# ax4.grid()\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.subplot_tool()\n",
    "plt.plot()\n",
    "plt.savefig('unseen_autocorrelation.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8))= plt.subplots(4, 2)\n",
    "fig.suptitle('compare time series patterns seperately')\n",
    "#---- point 1\n",
    "ax1.plot(range(plot_xvalues), point_1, 'o-')\n",
    "ax1.set_ylabel('simulated value')\n",
    "\n",
    "ax2.plot(range(plot_xvalues), point_1_pred, '.-')\n",
    "ax2.set_xlabel('time (s)')\n",
    "ax2.set_ylabel('predicted values')\n",
    "#---- point 2\n",
    "ax3.plot(range(plot_xvalues), point_2, 'o-')\n",
    "ax3.set_ylabel('simulated value')\n",
    "\n",
    "ax4.plot(range(plot_xvalues), point_2_pred, '.-')\n",
    "ax4.set_xlabel('time (s)')\n",
    "ax4.set_ylabel('predicted values')\n",
    "#---- point 3\n",
    "ax5.plot(range(plot_xvalues), point_3, 'o-')\n",
    "ax5.set_ylabel('simulated value')\n",
    "\n",
    "ax6.plot(range(plot_xvalues), point_3_pred, '.-')\n",
    "ax6.set_xlabel('time (s)')\n",
    "ax6.set_ylabel('predicted values')\n",
    "#---- point 4\n",
    "ax7.plot(range(plot_xvalues), point_4, 'o-')\n",
    "ax7.set_ylabel('simulated value')\n",
    "\n",
    "ax8.plot(range(plot_xvalues), point_4_pred, '.-')\n",
    "ax8.set_xlabel('time (s)')\n",
    "ax8.set_ylabel('predicted values')\n",
    "\n",
    "fig.tight_layout(pad = 2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Animation of complete predicted simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "\n",
    "\n",
    "def save_plot_predict_rf(first_row, last_row, data, timestep):\n",
    "    #subset the correct data\n",
    "    data = data.iloc[first_row:last_row,:]\n",
    "\n",
    "    #import libraries needed\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    # Create figure for animation\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cmap = cm.gist_yarg\n",
    "    im = ax.imshow(data, interpolation='nearest', cmap=cmap, vmin=0, vmax=0.275)\n",
    "    ax.set_title('Prediction timestep %i' %timestep)\n",
    "    fig.colorbar(im,cax=cax, orientation='vertical', extend = 'both', ticks= [0.05,0.1,0.15,0.2,0.25])\n",
    "\n",
    "    #save figure for animation\n",
    "    plt.plot()\n",
    "    plt.savefig(f'solution-{timestep}.png')\n",
    "    plt.close()\n",
    "\n",
    "last_row = 0\n",
    "first_row = 0\n",
    "\n",
    "# Create plots of simulation\n",
    "for timestep in range(steps):\n",
    "    # timestep is 0,1,2,3,4,5...\n",
    "    # first rows is 0+niks, 1+ last_row\n",
    "\n",
    "    last_row = first_row + vertical_pixels\n",
    "    save_plot_predict_rf(first_row = first_row, last_row= last_row, data=framed_predictions, timestep= timestep)\n",
    "    first_row = last_row\n",
    "\n",
    "with imageio.get_writer('unseen_anim.mp4', format='FFMPEG', mode='I', fps=3) as writer:\n",
    "    for i in range(steps):\n",
    "        image = imageio.imread(f'solution-{i}.png')\n",
    "        writer.append_data(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise decision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature importance based on decrease in impurity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature importance based on permutations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The following code block represents the visualisation of a tree based model. The random forest predictor as defined above.\n",
    "tree = rf.estimators_[1]\n",
    "# tree = rf.estimators_[50]\n",
    "\n",
    "# # Export as dot file\n",
    "export_graphviz(tree, out_file='tree.dot',\n",
    "                feature_names = feature_names ,\n",
    "                class_names = None,\n",
    "                rounded = False, proportion = False,\n",
    "                precision = 3, filled = True)\n",
    "#\n",
    "# # Convert to png using system command (requires Graphviz)\n",
    "\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png'], shell=True, cwd=os.getcwd())\n",
    "#\n",
    "# # # # Display in script\n",
    "\n",
    "Image(filename = 'tree.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
